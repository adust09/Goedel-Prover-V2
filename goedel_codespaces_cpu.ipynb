{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Goedel-Prover-V2 — Codespaces CPU Demo (JupyterLab)\n",
        "\n",
        "> Note: 8B model on CPU FP32 is heavy. Use a large Codespace (>=32GB RAM) and keep n/max_length small. For practical runs, generate in Colab GPU then compile/summarize here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, subprocess, json, os, pathlib\n",
        "print(sys.version)\n",
        "!python -V\n",
        "!pip -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "setup"
        ]
      },
      "outputs": [],
      "source": [
        "# Minimal Python deps for CPU inference + compile/summarize\n",
        "!pip install -U pip\n",
        "!pip install jupyterlab ipywidgets\n",
        "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install transformers accelerate tqdm pandas sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Hugging Face login\n",
        "Only needed if the model repo requires auth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from huggingface_hub import login\n",
        "# login(token=\"hf_...\")\n",
        "pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare a tiny input set\n",
        "Keep input extremely small for CPU demo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "src = Path('dataset/test.jsonl')\n",
        "dst = Path('dataset/test_small.jsonl')\n",
        "if src.exists():\n",
        "    with src.open('r') as fin, dst.open('w') as fout:\n",
        "        line = fin.readline()\n",
        "        if line:\n",
        "            fout.write(line)\n",
        "    print('Wrote 1-line sample to', dst)\n",
        "else:\n",
        "    print('Warning: dataset/test.jsonl not found. Please add a JSONL input.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPU Inference (very slow; high RAM)\n",
        "Uses `inference_cpu.py`. Reduce `--n` and `--max_length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "inference"
        ]
      },
      "outputs": [],
      "source": [
        "!python inference_cpu.py \\\n+        --model_path Goedel-LM/Goedel-Prover-V2-8B \\\n+        --input_path dataset/test_small.jsonl \\\n+        --output_dir results/codespaces_cpu \\\n+        --n 1 \\\n+        --max_length 256 \\\n+        --temp 0.2 \\\n+        --use_cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Lean toolchain (elan)\n",
        "Build `mathlib4` to enable compilation via Lean REPL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "lean"
        ]
      },
      "outputs": [],
      "source": [
        "# Install elan (Lean toolchain manager) — non-interactive\n",
        "!curl -sSf https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh | sh -s -- -y\n",
        "# Note: Lean binaries are installed under ~/.elan/bin/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "lean"
        ]
      },
      "outputs": [],
      "source": [
        "# Build mathlib4 (can take a while on first run)\n",
        "!bash -lc 'cd mathlib4 && ~/.elan/bin/lake build'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile generated codes via Lean REPL\n",
        "Limit CPU parallelism and proof timeout for stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "compile"
        ]
      },
      "outputs": [],
      "source": [
        "!PROOF_TIMEOUT=180 python src/compile.py \\\n+        --input_path results/codespaces_cpu/to_inference_codes.json \\\n+        --output_path results/codespaces_cpu/code_compilation_repl.json \\\n+        --cpu 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summarize results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "summarize"
        ]
      },
      "outputs": [],
      "source": [
        "!python src/summarize.py \\\n+        --input_path results/codespaces_cpu/code_compilation_repl.json \\\n+        --full_record_path results/codespaces_cpu/full_records.json \\\n+        --output_dir results/codespaces_cpu/summary\n",
        "\n",
        "import json, os\n",
        "meta_path = 'results/codespaces_cpu/summary/meta_summarize.json'\n",
        "if os.path.exists(meta_path):\n",
        "    print(json.dumps(json.load(open(meta_path)), indent=2, ensure_ascii=False))\n",
        "else:\n",
        "    print('meta_summarize.json not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notes\n",
        "- If CPU OOM occurs during inference, reduce `--max_length`, `--n`, and input size.\n",
        "- For practical usage: run inference on Colab GPU (quantized) and copy `to_inference_codes.json` + `full_records.json` here, then run compile/summarize cells only.\n",
        "- You can adjust `PROOF_TIMEOUT` and `--cpu` to control compile parallelism and timeout."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

